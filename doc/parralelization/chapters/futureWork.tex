% !TEX root = ../main.tex
\chapter{Future Work}
The results show, that it is possible to further speed up LoLAs performance. But they draw not the complete picture. Using different optimization methods and other Petri nets, the performance gain will most certainly ease noticeably.

To classify the extent of this behavior, further benchmarks should be evaluated. They will show if the observed scaling is applicable in a productive environment. Additional a memory benchmark might be of interest, that compares memory load with the mara implementation (depending on the chunk size) in comparison to the old implementation.

However, LoLAs potential for parallel and sequential execution is by far not exhausted. For example, the new implementation of the \inline{HashingWrapperStore} seems to be much slower than the previous version. As stated in chapter \ref{evalSingleThread}, a concept for a single-threaded use might decrease the execution time considerably. A special subclass that is specialized for - and only used on - single-threaded execution, might be a good way to achieve this goal.

As discussed in chapter \ref{codeBase} the implementation of the \inline{ParallelExploration} class seems to be based on the single-threaded base class. With this approach, all threads used for the exploration, have the same job. They are also accessing equally, the same global store. A more specialized approach might deliver even better scaling effects, or reduce the overhead that is needed for the synchronization.

For example, currently, the store is accessed via the \inline{searchAndInsert} method. As the name implies, this method not only searches the store for a net state - which requires reading access, it also inserts the state into the store if it was not found - which requires writing access. If it is possible to divide this two operations in a thread-safe manner, it might be possible to reduce the chance of blocking mutexes. Especially if we consider that the search depends on explored states, whereas inserting can be done in constant time.

Another possible improvement might be a master thread that queries for leftover work by worker threads, that can then be handed over to idle worker threads. This way it can be avoided to check for idle threads on each search cycle like it is done currently.

However, this would be solutions that are bound to the current structure of LoLA. It might be worth to completely reimplement the parallel search. The recent development in depth-first search is trying to introduce efficient parallel extensions of existing algorithms\cite{bloemen2016multi}\cite{holzmann2008stack}. With an algorithm and a data structure that is well tested and specifically designed for parallel use, it might be possible to achieve a better performance for more use cases.