% !TEX root = ../main.tex
\chapter{Conclusion}
Optimizing software is a common part of its development cycle, but the approach depends highly on the issue.

But a generally good order of actions is to first narrow down the performance issue and base implementation changes only on empirical test results after that. This approch might seem obvious, but during this work a general temptation was observed to 'just check if this idea works'. The change between the Pthread and the Compare and Swap synchronization illustrates that. It was done before any serious measurement because a problem with the synchronization was a likely cause for the bad thread scaling. As the data shows, this assumption was right in principle, but in practice another unexpected part of the synchronization was the cause. A better preparation could have saved work and time at this point.

Another part that could have been improved is the benchmarking approach. The third party tools where much more reliable and easy to use than the manual approach. At least in this work it can be said that the time spent to learn how to use these tools was well spent. With the help of this tools it was possible to find and eliminate a serious bottleneck.

The data produced by the tools lead to the mara integration in LoLA, which is able to utilize additional threads for a higher performance. Though the synchronization overhead prevent an early improvement, we saw that there exist petri nets which can be searched faster with this implementations and enough threads. Using more buckets can enhance this effect further. Additionally the new heap allocation strategy can improve the general performance of LoLA independent of the parallelization.

In contrast - as implied earlier - the switch between the Compare and Swap synchronization and the PThread synchronization has no major affect on the performance. The Compare and Swap algorithm tends to perform generally slower than the PTread mutexes. It seems that the system allocator is the root cause of the bottleneck.

The real life beninfit of the parallelization still has to be validated. but with the insights of this work LoLA can be understood better and we can deviate some work that can be done to further improve LoLA. 