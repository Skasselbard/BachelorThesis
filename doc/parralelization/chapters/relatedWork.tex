% !TEX root = ../main.tex
\chapter{Background and related work}
Before we start, we will look at the background of this work. We will see an overview of parallel computing and some challenges that arise with parallel computation. We will investigate some related work in parallel search algorithms, and settle some basic terms of performance measurement. At the end of this chapter, we will look at some relevant implementation details of LoLA, and at the hardware that was used for this work.

%TODO: Definition Ã¼berdenken
\section{Parallel computing}
Until the first decade of the 21st century, the computing power of processors could be scaled with higher clock frequencies. However, this trend was replaced by the tendency to add multiple processing units on recent processors. For this reason, software that wants to exploit all processing capacities of current hardware has to adopt some parallel computing paradigms.

Parallel computing can be described as the simultaneous execution of multiple related operations at the same time. There are different levels on which these operations can reside. Beside bit-level parallelism - where an arithmetic logical operation is executed on multiple bits (e.g. 32bit and 64bit architectures) and instruction-level parallelism\cite{wall1991limits} - where different processor actions (e.g loading memory, alu operations, writing memory) can be executed simultaneously, we will focus on the more abstract level of parallel tasks.

A task may be some arbitrary routine that can be processed on a processor. It can be thought of as a thread. Independent tasks can obviously be executed independent and therefore in parallel. But tasks that work as part of the same workflow or on the same data, might conflict with each other in some way. This is typically the case if we try to distribute an existing program to different subtasks that can be executed simultaneously. Some of these subtasks might depend on a result of a previous task or have to change globally shared data. This results in some problems that arise with parallel computing.

\subsection{Limits in concurrency}
There are limits to the potential of parallelizing tasks. Ideally, a program which is executed with $n$ processing units or threads will terminate in $\frac{1}{n}$ of the time. But programs tend to have some parts that have to be executed sequentially. For example: to write to a file it has to be opened previously.

Amdahl already formulated in the 1960's\cite{amdahl1967validity} that the speedup of a program is limited by its sequential parts. This implies that thread scaling reaches its limits with relatively achievable amounts of threads. From his work a formula was extracted that is known as Amdahl's law. 

Gustafson later reevaluated that formula\cite{gustafson1988reevaluating} and points out that Amdahl assumes a fixed problem size. This is often true in academic research, but practical programs scale the size of the problem with the number of processors. He introduces a new formula with a variable problem size and a much more optimistic scaling.

In both cases, it should be kept in mind, that a practical parallel program typically needs some kind of overhead to synchronize work that will prevent an optimal scaling. 

\subsection{Data integrity}
Another major concern in the design of parallel applications is the data access. Often different parts of a program need to read or write the same information. Doing this step by step is trivial, but if two parallel executed task trying to access the same memory regions concurrently, data corruptions quickly strike the careless.

While concurrent reading is typically unproblematic, a single writing task can interfere with all other tasks. Not only can it cause a reader to return an incomplete written data segment, also accurately written data might be bound to a different program state, which does not fit to what the reader assumed at the time the read was scheduled.

If such behavior is not handled correctly, the corresponding memory segment depends on the first arriving task and are therefore called race conditions. A more detailed analysis of that term was done by Netzer et. al.\cite{netzer1992race}.

To assure data integrity of shared memory, some kind of synchronization has to be done. This is typically achieved by blocking the access to the memory (especially operations involving writing) to one thread at a time. This necessity supports the development of some general synchronization concepts.

\section{Synchronization concepts}
Global memory with access restrictions can introduce bottlenecks which quickly degrades the whole program to sequential (or worse) speeds if done too frequently. One way to avoid that is to allocate thread local memory. This is genrally advised for data that depends on a single thread anyway, however often there is some global memory that has to be shared between threads.

To keep this memory upright some rules to access it are required. An often used way is to simply block the access so that it can only be used by one thread at a time. We will look at some of such concepts here before we will come across them later:
 
A simple lock could be a flag with the two states \inline{locked} and \inline{unlocked}. Memory protected by such a lock would be accessible only if the flag is set to \inline{unlocked}. Keeping the integrity of the states would be the duty of the implementor.

A more advanced version of such a lock was introduced by Dijkstra\cite{dijkstra1968cooperating} and are called \textbf{semaphores}. Semaphores assume an amount of a resource that can be distributed. It has a value that can be incremented or decremented. If it is decremented to zero, the next task that is trying to access the resource has to wait until another task increments the semaphore again. For example, the set of keys of an apartment can be such a resource where the available count is a semaphore. If someone leaves the apartment he or she will want to take a key from the set to be able to come back again. By taking a key, the left amount is decreased. If there is no key left and somebody wants to leave the apartment (and come back later) this person has to wait until somebody else returns a key.

If there is only one resource available (a semaphore with the maximum value of one) the semaphore is equivalent to the previously described lock. Such a construct is typically referred to as \textbf{mutex}.

A locked resource met by a thread can force a thread to delay its work. If the thread cannot be suspended, some sort of waiting has to be implemented. A trivial algorithm of that kind just loops until the resource is freed and blocks the resource again immediately. Such a behavior is called \textbf{spinlock} because of its cycling behavior.

This rather trivial algorithm already introduces a quite concealed race condition. A possible chance exists that two threads examine an unlocked mutex and upon that, access the underlying resource before it could be locked by the contender. To resolve such problems, atomic operations were developed. Atomic operations are guaranteed to be executed completely, or not at all, without any possible remote influence. One of them is called \textbf{compare and swap}. It will compare a resource with an expected value and will change the resource on equality. Atomic operations are often implemented in state of the art hardware for maximum performance.

\section{Depth-First search parallelization}
LoLA uses a kind of depth-first search over a Petri nets possible (reachable) states, to evaluate a predicate. A depth-first search discovers graphs\cite[chapter~1]{bondy1976graph} by traversing the first possible edge of the current vertex. If no unknown edge is left for the current vertex, backtracking\cite{golomb1965backtrack} is used, to find a previously discovered vertex with untraversed edges. The graph is completely discovered if no traversable edges are left.

As stated before, Petri nets are analyzed by LoLA, by searching for a set of net states that satisfy a given property. This means that one of the primary (possibly the single important) computation of LoLA is the depth-first search (beside some optimizations like net reduction techniques). Parallel execution of the search might imply parallel execution of LoLA. For this reason, utilizing a parallel depth-first search is much desired.

However, parallelizing depth-first search is not trivial. Freeman summarizes that it, in fact, was long thought to be inherently sequential\cite{freeman1991parallel}. He gives an overview of the breakthroughs and history of depth-first search, and  directly reviews the history of parallel algorithms. Those often exploit a specific trait of the given problem. For example, Smith gave an algorithm that works for planar graphs\cite{smith1986parallel} and Roa et. al. gave an algorithm for disjoint graphs\cite{rao1987parallel}. Another algorithm form Aggarwal et. al. exists that works on general directed graphs\cite{Aggarwal:1989:PDS:73007.73035}, but it assumes that the complete graph is already known before the search is initiated.

LoLAs sequential approach is based on "strongly connected components (SCC)" introduced by Tarjan\cite{tarjan1972depth}. His algorithm has the great benefit that the graph to search - can be discovered - during the search. This not only gives the chance for an early abort once a property is satisfied, without loading the complete graph. It also allows the search on possibly infinite graphs. Losing these qualities would significantly limit LoLAs usability.

Others value the same convenient properties and so Bloemen et. al. recently provided an algorithm based on Tarjan's SCC's that preserve them\cite{bloemen2016multi}.

\section{Performance measurement}
\label{benchmarking}
\subsection{A distinction between different views on software}
Comparing the performance of different implementations - like we want to do later, can be a bit more complicated as it might appear initially. Just like that, what is called a performant application is highly dependent on its environment. 

A common characteristic is the time it takes to execute a program or a subroutine. But in a lot of scenarios, the time consumption actually does not matter that much. A user might not even notice a factor of 1000 between routines that operate in an order of microseconds. At the same time, he might worry that the program can access enough memory to finish its task. That means before measuring the performance of a program or subroutine, one must define the characteristics that define the performance of exactly that given program or subroutine. Other characteristics beside execution time and memory consumption can be network latency, energy consumption or responsibility. Ultimately it is everything that increases the usability of a program.

After knowing what will be measured, the next concern is how it will be measured. The act of measuring is typically called benchmark and the approach here is influenced by the scope of the measurement. The java community describes a general distinction between macro, meso and micro benchmarks\cite{oaks2014java}. Each type addresses a different scope of a program.

\textbf{Macro-benchmarks} give an overview over the entire execution. The data collection is often less complicated since it is done over the whole application runtime and hardly requires any modifications to it. Measuring the runtime of a program is as simple as it gets: the time difference between the end and the start of the program. Memory allocation can often also be measured with simple tools that the operating system or other third party software provide. The downside is that a lot of software cannot be measured at all in this scope. So it is a property of event-driven programs that they do not terminate at a fixed point in time and therefore the execution time is of limited use.

\textbf{Micro-benchmarks} address the most narrow aspects of an application. They measure the execution of single instructions or atomic routines. Results of micro benchmarks have to be taken with a grain of salt because compile and hardware optimization strongly influence the results. Assumptions that work for the single code snippet might become false after the compiler integrates that part into the complete program or vice versa. It is a good advise to use a benchmarking tool which is built for this task. Overwise there is a risk to be trapped in false results. Like compilers that are optimizing away whole loops (which in turn might be the benchmark itself), because the result can be evaluated at compile time. The advantages of micro benchmarks, however, is that different implementations of basic algorithms can be compared directly. Provided they are performing equivalent in the macro scope of the program.

After the previous scopes described a detailed view and an overview, the last scope has to be something in between. And this holds true for the so-called \textbf{meso-benchmark}. Perhaps the best explanation for this kind is, that a meso-benchmark is what is not macro nor micro-benchmark. Meso-benchmarks deal with parts of a program that gets more complex. Like Modules or further reaching subroutines. These can be the most interesting benchmarks since they can narrow down bottlenecks to specific parts of the software. Unfortunately, they often require manipulating the program in some way. Which again means that the measured performance differs from the actual performance.


\subsection{Methods of measurement}
\label{BenchMethods}
For the actual measurements, two basic approaches can be distinguished: manual implementation or usage of a tool.

The quickest and most precise approach is to make own measurements. Self-written and therefore known software can be easily extended e.g. by counters for variables or measurements of time periods. All measurements can be tailored to the individual use case and results can be exported in the most convenient format. On the over hand, this approach can come with several downsides. Some of them might be that the maintenance of the measurements can get increasingly complex the more numerous they get, added code will influence the performance of the software itself, and own mistakes might hide behind seemingly satisfying results. There are probably other reasons that can be thought of. But short and temporary measurements of this kind might give o quick overview other the own software.

Probably the safer approach is the usage of a tool. There exist a variety of tools which are build for the inherent purpose to measure application performance with common metrics and use cases. Since the metrics reflect some characteristics of an application - a profile - these tools are typically called \textbf{profilers}. Using established profilers for performance measurement can help avoiding common traps and mistakes especially when they were designed by experienced developers who are skilled in this area. But it also means to invest time and effort to learn how to use them and how to interpret their results.

Profilers aid automate processes that otherwise would have to be implemented manually. An often used method to figure out bottlenecks is to take samples of the call stack during program execution. The calls that are most frequently sampled, will correlate with the program parts, where the most time was spent in. Another method is to use code injections. Here, the code is inserted automatically into the source or binary code at relevant spots. The injections are then used to count events or measure time intervals.

With the general techniques in mind, we can proceed to the actual software and its characteristics that will be examined in this work.

\section{Implementation details}
\subsection{Code base}
\label{codeBase}
This work is based on a previous attempt by Gregor Behnke to use multiple threads for LoLA. For this reason, his code will be taken as a base implementation for the parallel search. In the LoLA project structure, this would correspond to the \inline{ParallelExploration} class in src/Exploration.

This class itself seems to be based on a single-threaded algorithm in the \inline{DFSExploration} class, since the basic structure and several lines of code, as well as comments, are equivalent. The base algorithm was then altered by Behnke to make data manipulation thread safe. This implies that no specialized depth-first search algorithm was used, but the existing algorithm was extended.

The purpose of the exploration class family in LoLA is to keep track of the \textbf{paths} that were discovered during the search. The actual net \textbf{states} that are the vertices of the path are managed by another family of classes: the store classes. In more detail, this means that the exploration class will store the edges that lead to the current net state and chooses which edges of the graph will be used next to discover another net state. The discovered net states will be handed to the store class. It will tell the exploration class if this net state was already discovered earlier. If that is not the case the net state will be stored permanently

Behnke tried to implement multithreading (inside the exploration class) by exploring a different path with each thread. He, therefore, introduced thread local variables that keep track of the path, determine which edge will be expanded next and what the current net state is. The store is globally shared. Thread safe searching and adding to the store must be implemented by the store itself for this approach. The crucial work of Behnke inside the exploration algorithm was to distribute the work over all threads and keep the data synchronized.

Load distribution is done with a simple approach. If a thread can expand multiple edges and an idle thread exists, one edge will be discovered by the current thread and another will be discovered by the next possible idle thread. The data will be synchronized, so that the woken up thread will copy the previously discovered path of the waken. Cases like: what should be done if a thread has no more work (accessible edges) left or what to do if the current net state satisfies the asked property are also handled.

The store which is used by LoLA in the default case (\inline{PluginStore} ) does not provide any thread safety. The thread number is completely ignored within the relevant parts and so the behavior of this store when using multiple threads is undefined. But there is another store implementation which implements several buckets to store the states in: the \inline{HashingWrapperStore}. The buckets should only be accessible by at most one thread. The net states that should be stored are now given a hash value and every bucket has a range of hash values associated with them. With this solution multiple searches can be issued, as long as the hash value of the searched net states differ.

The actual classes which are used at runtime to expand the search tree, are determent by the call switches which are handed to the LoLA executable. The relevant switches for the parallel exploration are \inline{--threads=[threadCount]} and \inline{--bucketing=[bucketCount]}. A thread count greater than 1 will tell LoLA to use the \inline{ParallelExploration} class and the bucketing switch will signal to use a hashing store.

\subsection{Data synchronization}
To keep global data consistent, a synchronization method is needed. A common way to achieve this, is to restrict the access to one thread at a time. For this, the concept of mutexes and semaphores are often used as locks for access control. The downside of this approach is a bottleneck at the global data. If access is granted to at most one thread, every calculation that is done while accessing is also done at most with the speed that a single thread can provide. Therefore accessing these regions (including locking and unlocking them) should be done as infrequent and as quick as possible.

Since LoLA computes a lot of short and low level-instructions, locking and unlocking data segments might sum up to a significant amount of time. So the locking mechanism used by LoLA might also be a good starting point to search for a reason for that unexpected performance that LoLA shows.

Behnke used locks from the c++ pthread library. A possible substitute would be an own implementation with a Compare and Swap instruction as spinlock. This has the potential to keep the overhead for the locks at a bare minimum.

\section{Environment}
\label{env}
The used development environment consists of two machines. The general development is done on a virtual machine (VM). It runs with 4 threads on an Intel(R) Core(TM) i7-4770S CPU @ 3.10GHz with 4 physical cores. Each core supports hyperthreading which makes a sum of 8 threads on the host of the VM. A total of 17.4 GB of physical memory is available for use inside the VM. Kubuntu 17.04 is running as operating system.

As a performant test system, a server (ebro) was used. Ebro has 4 AMD Opteron\texttrademark 6284 SE processors. Each with 16 cores and 2.7GHz base clock with a max boost of 3.4GHz\cite{AMDSpecs}. This sums up to 64 physical cpu cores (and threads) in total. As physical memory, 995.2 GB are installed. The operating system is CentOS Linux 7 Core (collected with the hostnamectl command).

If no other source is provided the data was collected with the proc filesystem provided by the linux distributions.
\begin{center}
    \begin{table}[H]
        \centering
        \begin{tabular}{ | l | c | c |}
            \hline
            \textbf{Spec Name} & \textbf{VM} & \textbf{Ebro} \\ \hline
            Max. Clock per Thread & 3.1GHz & 3.4GHz \\ 
            Available Threads & 4 & 64 \\ 
            Memory & 17.4 GB & 995.2 GB\\ 
            Operating System & Kubuntu 17.04 & CentOS 7 Core \\ \hline
        \end{tabular}
        \caption{Specifications of the two development systems}
    \end{table}
\end{center}

% - ebro\\
% model name      : AMD Opteron(tm) Processor 6284 SE
% cpu MHz         : 1400.000
% 4 cpus 16 cores 16 threads
% MemTotal:       1043610248 kB

% - vm\\
% model name      : Intel(R) Core(TM) i7-4770S CPU @ 3.10GHz
% cpu MHz         : 3100.006
% MemTotal:       18281952 kB
% SwapTotal:       2097148 kB
