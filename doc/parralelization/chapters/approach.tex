% !TEX root = ../main.tex


\chapter{Approach}
After some insight in the underlying matter, we will now try to figure out where the actual bottleneck resides. We will try to change the underlying synchronization mechanism and see if the change has any effect. We will also rework LoLAs heap allocation strategy. At the end of the chapter we will evaluate the results of different benchmarks.

\section{Switching The Synchronization}
The first step taken was switching from the pthread library to a compare and swap (CAS) algorithm. Since C++ 11 there is an equivalent implementation in the standard library called \texttt{bool std::atomic::compare\_exchange\_weak(T\& expected, T val)} (or \texttt{bool std::atomic::compare\_exchange\_strong(T\& expected, T val)}) that was used for this task. This function compares the current value of an \texttt{std::atomic} with \texttt{expected} and replaces it with \texttt{val} if the comparison returns true. If it returns false it replaces \texttt{expected} with the actual value of the atomic. The weak version is allowed to return false in favor of a general performance gain, even if the compared values are actually equal.

To represent a lock that can either be locked or unlocked, a boolean as value is sufficient. To shape a spinlock like mutexes and semaphores with a CAS function, they have to loop until the expected value compares equal. There are two reasonable modes to lock: the first is to just wait until an observed lock is unlocked and the second is to wait for an unlock with an immediate locking. The first approach can be helpful to suspend the execution of the current thread until an external thread is signalling a continuation. The second approach can be used to block access to a resource until all necessary manipulations are completed. The resulting implementation is shown in listing \ref{CASIMPL}.

Swapping the old pthread implementation with the new CAS implementation now remains a matter of search and replace. The equivalent of the mutexes \texttt{pthread\_mutex\_lock()} is \texttt{waitAndLock()}. \texttt{Pthread\_mutex\_unlock()} corresponds to \texttt{unlock()}. The previous \texttt{sem\_wait()} correspond to a \texttt{waitForUnlock()} call after a \texttt{lock()}. And \texttt{sem\_post()} corresponds to \texttt{unlock()}.\\
However, this is a very specialized replacement which acts as a proof of concept. Other parts of the code might have to be replaced in another way, depending on the semantics of the part. Additionally the CAS implementation is as short as possible. The pthread library comes with some important features like a mechanism to reduces the risk of deadlocks and different modes for the semaphores.

\lstset{language=C++,caption={Basic implementation of a spinlock with compare and swap},label=CASIMPL, frame=none, stepnumber=5, backgroundcolor=\color{verylightgray}}
\begin{lstlisting}
#include <atomic>

enum LOCK{
    LOCKED,
    UNLOCKED
};

static inline void waitAndLock(std::atomic<bool>* lock){
    bool expected = UNLOCKED;
    while (!lock->compare_exchange_weak(expected,LOCKED)){
        expected = UNLOCKED;
    }
}

static inline void waitForUnlock(std::atomic<bool>* lock){
    while (lock->load() != UNLOCKED){
        continue;
    }
}

static inline void lock(std::atomic<bool>* lock){
    lock->store(LOCKED);
}

static inline void unlock(std::atomic<bool>* lock){
    lock->store(UNLOCKED);
}
\end{lstlisting}

\section{Finding the bottleneck}
\subsection{Benchmark characteristics}
We have now an application with a bottleneck and a possible solution to fix that bottleneck. Next we will compare the performances.

In this case, performance means execution time. The expectation is that the execution time decreases with an increased amount of threads. The time is therefore the characteristic of interest.

As test systems both machines described in \ref{env} were used.

As test data a predefined Petri net of the dining philosophers was used. It is a common concept in theoretical computer science to illustrate problems and risks of parallel processes and was originally introduced by Dijkstra\cite{dijkstra1971hierarchical}. The philosophers count can be scaled to an arbitrary amount. This is useful to increase the complexity (and therefore the execution time) of the net to a convenient level. The graph to search is also reasonably branched to allow a parallel discovery. The actually used net is a version with one thousand philosophers.

LoLA is executed with the \texttt{--threads=[threadCount]} and \texttt{--check=full} switches. The first switch simply sets the number of threads that should be used for the state exploration. The second switch cause LoLA to explore the whole state space without exploring a property. This ensures that the application will terminate after all states have been discovered and no varying discovery paths can influence the execution time.

\subsection{A general performance survey}
LoLA will spend most of the time inside the depth-first search of the state exploration. For this reason a simple execution with the given parameters will uncover if the different synchronization implementations have an impact on the performance. This would correspond to the macro benchmark scope and can be achieved without any additional modification or tools, because LoLA already measures its own execution time.

Unfortunately a simple test run on the VM reveals that there is no performance gain. Figure \ref{firstMacroBenchmark} shows that the new implementation is actually slower than the previous. A relevant speedup would divide the execution time by an order of the used threads. Taking this expectation as basis, both implementations still can be considered (roughly) in the same order of performance. The new implementation is needs about twice the time, while a quarter of the time was expected. Thus, we can conclude that we either missed the cause of the bottleneck, or the new implementation faces similar challenges.

\input{pictures/MacroNOBucketsSimple}

Since the benchmark result shows no positive change in performance, an exhaustive evaluation is dropped in favor of a detailed bottleneck analysis. In the next section we will use a more precise benchmark method to inspect the search characteristics.

\subsection{Searching for inefficient application components}
To develop a deeper understanding of the internal processes, we have to examine the individual program parts. The most important one remains the search for net states which is done in the \texttt{ParallelExploration} class.

LoLA will call the \texttt{depth\_first()} method of the \texttt{ParallelExploration} to initiate the search. There, all search threads will be initialized, started and destroyed (after they finished their work). Starting and destroying the threads takes constant time per thread. Since we will work with very few threads in relation to the number of states we will handle, these parts are insignificant for the performance measurement and can be ignored. But each thread will execute the \texttt{threadedExploration} method. This method is the algorithm for the actual search. It will loop until a given predicate is satisfied by any thread. For these reasons we will focus our measurements on this method.

With this we now know a precise code section that we want to inspect. Next we have to choose a new measurement method. We can decide between two general approaches: we can use an existing tool that can hopefully inspect the application parts we are interested in, or we can extend our code manually. Both approaches have their up and downsides. In our case we decided to use the manual approach. First because the interesting code is relatively short and clear and second because it can be done right away, without spending much time on learning a third-party software.

In the next step measurements have to be inserted into the relevant sections. However, what section is relevant cannot be known before measuring. We have to choose parts that seem to be most likely. This is an inherently subjective process, but some factors will be influential. For example, elemental assignments like \texttt{x = 5;} will take an insignificant amount of time, whereas function calls and loops can take arbitrarily long times to be executed. The cost of a bad selection is very low, since the measurements can be easily changed.

We measured the time with the functions provided by \texttt{std::chrono} from the c++ standard library. The exact code is considered trivial and will not be discussed further.

Table \ref{ManualBenchmark} is listing the performance of the code pieces that are considered relevant. Other less important measurements are omitted to keep a clear view. Here is an explanation of the values:
\begin{itemize}
    \item Total Thread Time - Cumulative time spent in each thread.
    \item Synchronization Time - Time spent to synchronize the threads with mutexes.
    \item Store Search Time - Time spent inside the \texttt{searchAndInsert} call. A method to safe the discovered states.
    \item Idle Time - The Time each thread spent for the \texttt{restartSemaphore} to be unlocked
    \item Work - Time that each thread spent while having states left that can be discovered
    \item No Work - Time spent during waiting for new states that can be expanded including reinitialization. 'Work' and  'No Work' should cover the complete search loop.
\end{itemize}

\begin{center}
    \begin{table}[H]
        \centering
        \begin{tabular}{ | l | c | c | c | c |}
            \hline
            \textbf{} & \textbf{Trhead 0} & \textbf{Trhead 1} & \textbf{Trhead 2} & \textbf{Trhead 3}\\ \hline
            Total Thread Time & 256s & 256s & 256s & 256s \\ 
            Synchronization Time & 0.00000004s & 0.00000004s & 0.00000005s & 0.00000005s \\
            Store Search Time & 58s & 58s & 56s & 58s \\ 
            Idle Time & 0.22s & 0.22s & 0.22s & 0.22s \\ 
            Work & 64s & 64s & 62s & 64s \\ 
            No Work & 0.5s & 0.5s & 0.5s & 0.5s \\ \hline
        \end{tabular}
        \caption{Manual meso benchmark of the \texttt{ParallelExploration}}
        \label{ManualBenchmark}
    \end{table}
\end{center}

\subsection{Following the hints}
The benchmark results are quite inconsistent. On the one hand they give some insight where the bottleneck might be. On the over hand the values do not add up correctly.

Most problematic is that the time inside the search loop (sum of 'Work' and 'No Work') is not anywhere near the total time spent inside the search. This is very unexpected because almost all the time must be spent there. An unlisted measurement of the time before the loop also could not uncover a missing gap. This is a strong hint that this approach is not applicable in our scenario.

However, even though the results have to be taken with caution, we can take two hints from them. First, the time spent inside the mutexes seem to be insignificant. Whereas the time spent to store the discovered states seem to cause nearly all the work inside the loop. The corresponding method - \texttt{searchAndInsert} - is called by all threads directly on the globally shared store.

What store is used is decided at runtime. A trivial debug session can uncover that the \texttt{PrefixTreeStore} is used with the parameters we pass to the LoLA call. Although the thread id is passed to the method a look to the signature:
\lstset{language=C++, frame=none, caption={\texttt{searchAndInsert} signature}, stepnumber=5, backgroundcolor=\color{verylightgray}}
\begin{lstlisting}
bool PrefixTreeStore<T>::searchAndInsert(const vectordata_t *in, bitarrayindex_t bitlen, hash_t, T **payload, threadid_t, bool noinsert)
\end{lstlisting}
shows that the id is completely ignored (no variable name is defined). This means that the multi-threaded behavior of this class is completely undefined and it should not be used in this scenario. We have to change the way we call LoLA, so that we can assure a thread safe behavior.

Fortunately there is a way to use an appropriate store implementation. If LoLA is called with the additional parameter \texttt{--bucketing=[numberOfBuckets]} the \texttt{Hashing\-Wrapper\-Store} store is used instead of the \texttt{PrefixTreeStore}. The \texttt{Hashing\-Wrapper\-Store} divides the memory into different buckets and uses hashes of the discovered states to store them. Each bucket is used for a range of hashes and the access to a bucket is only granted to at most one thread at a time. With this implementation multiple threads can store their discovered states at the same time, as long as the states hashes differ. Because of this insight all future calls of LoLA will be done with the \texttt{--bucketing=[numberOfBuckets]} parameter.

Unfortunately the new parameter has no relevant impact on the performance of LoLA. Additionally measuring further timings inside the \texttt{Hashing\-Wrapper\-Store} lead to even more inconsistencies. This caused a greater lack of trust and an overthinking of the benchmark method.

\subsection{Reconsidering the approach}
Our previous results point to a bottleneck inside the \texttt{searchAndInsert} method of the store. But due to inconsistencies between them they seem unreliable.

The exact reason is unknown and to find the cause might be as difficult as the search for the bottleneck itself. But during the implementation of the benchmarks two problems arose that could be related:

First it proved difficult in practice to measure exactly those periods of interest. Especially covering the whole implementation is quite error prone because the implementation is not linear. There are several branches, loops and early returns that have to be considered. As a result, one might start a time period more often than to stop it or vise versa. This quickly leads to wrong times without noticing it.

The second problem refers to the kind of time that is measured. Especially in multi-threaded environments a distinction between real-time (or wall-clock time) and cpu time is important. The real-time of a program is the time that passes for an observer during its execution while the cpu time is the time the cpu was active. This means that the cpu time can be actually lesser or greater than the real-time. For example a program that is temporarily suspended for another process can have a shorter cpu time than real-time. Whereas a program that is executed on multiple cpus will have its cpu time also multiplied by the number of cores (or threads), resulting in a greater cpu time than real-time. In our scenario, blocking a thread with a mutex might influence the time periods that are taken. To measure correct values, a deep understanding of the matter is necessary and the approach should be well-thought-out.

Since the time for this work is limited and the author is a novice in the field of benchmarking, the manual approach is put aside for a more refined third-party solution.

\subsection{Accepting help}
If a challenge gets to difficult to face it alone, getting help is something worth considering. In our case, the difficulties and the trust in the own approach depleted so much that the use of a well-designed tool seems to outweigh the effort that has to be spent to use one efficiently.

But before learning to use a profiling tool, we have to know which one we want to use. There are countless profilers to choose from, making an exhaustive evaluation impossible in the scope of this work. As such it was decided to select some candidates from an internet research, that are mentioned often in similar environments.

In the following a short overview of the candidates shall be given with a reason that this candidate will or will not be used. This will be no detailed comparison with a conclusion why the choice is the best in our case. Instead it should only give insight on why the choice was made.

\subsubsection{Gprof}
Gprof is a profiler that was developed in the 1980's. It generates a flat and a call graph profile\cite{graham1982gprof}. In the flat profile is listed how often a routine was called and the cumulative time spent in it. The call graph lists which routine calls another and by which itself was called.

The profiler is integrated into the gcc compiler. Passing the \texttt{-pg} switch to gcc will add code to the compiled executable that is needed for profiling. A program that is compiled in this way can be executed normally, but during execution it will gather data that is later be written to an output file.

Reading the documentation will reveal an additional precondition: the program has to be closed with the \texttt{exit} function. In most cases this is no problem since it will be called implicitly as soon as the \texttt{main} function exits normally. LoLA however, calls the \texttt{\_exit} function explicitly. This saves a lot of time during the teardown of the program because the destructor of allocated objects is not called. Instead the operating system just frees the allocated memory.

It is quite easy to change the behavior of LoLA to call the default \texttt{exit} function. But it causes LoLA to be unresponsive at the end of the execution for at least a very long time. And because its cleanup process was designed to use the 'dirty' approach, it is not guaranteed that it will return at all. Additionally an attempt with a modified LoLA executable and a simple Petri net produced an empty output file.

As a result, gprof is dropped as a candidate at this point to move to next candidate.

\subsubsection{Valgrind}
"Valgrind is a dynamic binary instrumentation framework designed for building heavyweight dynamic binary analysis tools"\cite{nethercote2007valgrind}. Dynamic binary analysis is used to "analyse programs at run-time at the level of machine code". The code that is needed for the analysis "is added to the original code of the client program at run-time" which is what is called dynamic binary instrumentation.

This means that valgrind can inject code into existing applications and recompile the whole program. What code should be injected can be defined in a plugin for valgrind. A widely known plugin is "memcheck". With its help, inconsistencies of allocated memory can be discovered. Valgrind also publishes the "callgrind" profiling plugin that is able to generate call graphs of an executed application.

The code injections and recompilation of valgrind have a great impact on the performance of the analysed program. For example the speed of an application analysed with memcheck is reduced by a factor of 10 to 30 \cite{valgrindTools}. But an even more problematic detail for our scenario is, that parallel programs will be executed serially. Some problems and behaviors like race conditions will be obscured this way. And most important: the speedup that a parallel program can achieve over a sequential one cannot be observed. This makes valgrind inapplicable for our performance measurement.

\subsubsection{Perf}
Perf is a profiler that is directly integrated into the linux kernel. It uses hardware performance counters. These are unique registers on the processor, that count certain events like cache misses or branch mispredictions. This OS and hardware proximity, make the use of perf very cheap in execution time. The accumulated data however, can get quite big very fast.

The documentation of perf leaves a lot to be desired, but the usage is reasonable straightforward and produces well-organized data. It is easily possible to trace for bottlenecks down to the (with debug symbols annotated) disassembly. But interpreting the data demands some effort from the user. For example one must know how to read the disassembly and how to find the corresponding sections in the source code.

Perf can be configured to count and measure innumerous events. Understanding the complete complexity would be way out of scope for this work. But with a bit of searching, guides can be found that introduce perf calls and explain what they are doing\cite{perfExamples}.

Testing and refining these examples gave some usable data that was used as a basis for further modifications to LoLA. In the next section we see how perf was used and what data it collects.

\subsubsection{Use of perf}
The examples which were used to get to know perf, uncovered a new possible bottleneck quickly. But before we look at the data, we should take a look at the used perf call and its effect. The actual measurement was done on the vm since special rights are needed for execution.

The basic perf command that is used to collect data is \texttt{perf record}. It can be executed with every possible bash command or delegated to a process id (pid) and is normally configured with an event that should be measured. We will only use the "stack chain/backtrace" feature to generate a call-graph. With this option perf periodically takes (samples) stack traces which can be used to analyse which functions were called most, from which function they were called and which functions they were calling. The functions that are sampled most of the time correlate with the functions that are responsible for the most cpu load (if enough samples are taken). The corresponding perf switch is \texttt{--call-graph}. The switch itself has three modes which determine how the data is collected. We use the \texttt{dwarf} mode since the man page states that the others are either discouraged on gcc programs or require special hardware.

It is also possible to limit the perf call to a time interval with the \texttt{sleep n} parameter. It will cause perf to abort after \texttt{n} seconds. This comes in handy to reduce the execution time of lola and to limit the data size generated by perf (which can grow easily in the GB dimensions).

The final bash call that was used looks like this:
\lstset{language=bash,caption={Profiling lola with perf}, frame=none, stepnumber=5, backgroundcolor=\color{verylightgray}}
\begin{lstlisting}
    lola --check=full --threads=4 --bucketing=10000 phils1000.lola&;
    PID=$!;
    sudo perf record -p $PID --call-graph dwarf sleep 30;
    kill $PID
\end{lstlisting}
With \texttt{lola --check=full --threads=4 --bucketing=100 phils1000.lola\&;} LoLA  is executed in the background, \texttt{PID=\$!;} stores the most recent background program in a PID variable, \texttt{sudo perf record -p \$PID --call-graph dwarf sleep 30;} executes the perf profiler for 30 seconds and \texttt{kill \$PID} kills lola after perf returns.

To analyze the data, perf provide the perf report command. It will process the gathered data and output an interactive data structure like in figure \ref{perf_record_sample}. In the output we can see what functions are sampled most of the time ('Self' column), in which functions children where taken the most samples ('Children' column), the command name that was executed, the library in which the function resides and the name of the function. A more detailed explanation of the different columns and additional features of the perf report command can be found in the man page.

Knowing the basics for the perf usage, we can now look closer into the generated data and make more reliable assumptions about the possible location of the bottleneck.

%ergebnis
\subsection{Result and Conclusion}
\label{first_perf_results}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{pictures/perfReportSample.png}
    \caption{perf report output}
    \label{perf_record_sample}
\end{figure}
Figure \ref{perf_record_sample} shows that over 95\% of the samples were taken somewhere in the \texttt{threadedExploration}. This is expected because it is where the search of LoLA is implemented and almost all what LoLA does is searching. We can also see that nearly half of the samples were taken inside the \texttt{searchAndInsert} method of the global store. This observation corresponds to the hint we took from the manual benchmark approach. The different \texttt{searchAndInsert} methods are caused by the implementation of the \texttt{Hashing\-Wrapper\-Store} which associate each bucket with a \texttt{PrefixTreeStore} and forwards each \texttt{searchAndInsert} to the corresponding \texttt{PrefixTreeStore}.
The new information that we get from the data is that the most samples inside the \texttt{searchAndInsert} methods are taken in a \texttt{\_\_lib\_calloc} call. Almost half the samples of all taken originate here. They are almost certainly caused by allocating memory on the heap with \texttt{malloc}, \texttt{calloc}, or \texttt{new}. This makes perfect sense, since the purpose of the \texttt{searchAndInsert} method is to store newly discovered net states, which has to be done on the heap. In fact a simplified high-level view of the search could be described as:
\begin{enumerate}
    \item Discover a net state
    \item Store the net state
    \item Repeat until all states are discovered
\end{enumerate}
Discovering net states is done by firing a transition from the fire list (a list of all transitions that can be fired from the current net state). Beside the \texttt{searchAndInsert} method we can see this behavior in the profiling data too. Getting the fire list is the second most sampled method inside the \texttt{threadedExploration} and firing the transition was also sampled often enough to account for almost 1\% of all samples.

The calls inside \texttt{libcalloc} are a variety of system calls. Understanding what they really do is outside of the scope of this work. But the last sampled functions containing words like 'lock' or 'wake'. These are probably used to synchronize threads and lock memory regions. Ultimately the allocator has to be thread safe too.

Allocating heap space in high frequencies is no common use case, thus the assumption that the allocator just blocks until a request was fully processed seems to be reasonable. Additionally there is only one system allocator for multiple threads. If they all call the same allocator it is likely that they get in each others ways with higher frequent calls and with an increasing amount of used threads.

With perfs data and the previous assumptions we can be quite confident that changing the way we allocate heap space, can have an impact on LoLAs performance. For example we can reduce the frequent and complex calls to the allocator, by using bigger preallocated chunks of memory and manage them by our selves. This would not only reduce the calls to \texttt{libcalloc} needed, it also has the potential of reducing the overall consumed memory, since we already store pointers to each net state inside the global store. The fact that we store them permanently until LoLA is terminated makes it unnecessary to manage additional data for defragmentation handling. This information can get quite big in the system allocator, especially when allocating a lot of small data like LoLA does. The chunks can also be associated with each thread or each bucket of the \texttt{Hashing\-Wrapper\-Store} to make their access thread safe. This way we can access multiple memory locations by multiple threads. If thread synchronization is really a problem by the system allocator like previously assumed, this approach potentially resolves the issue of the general sequential performance of LoLA with the parallel implementation.

An implementation would require a custom allocator that wraps the system calls. Fortunately we have access to exactly such an allocator from a previous project at our chair called 'mara'. The participation of the author of this work in the development of mara is another advantage. The results of this circumstances lead to an integration of mara into LoLA for the net state search.

\section{Allocation strategy change}
In the last chapter we saw that the profiling results lead deeper into the \texttt{search\-And\-Insert} method. The data suggest that high frequent calls to the system allocator might be the cause of the bottleneck. To see if this assumption is true, we have to change the way LoLA uses the heap and measure the performance again.

We will use the Memory and Resource Allocator (mara) to wrap calls to the system allocator. Mara was previously developed at the chair of theoretical computer science as a student project by Julian Gaede, Marian Stein and Tom Meyer. The later use in LoLA was a part of the project goals.

A major part of maras constraints was that once allocated memory, will never be rearranged and never be freed until program termination. Thus mara is allowed to drop references that were previously shared. It can be freed later by the operating system. This allows mara to reduce the overhead for heap allocation in space and time.

Mara publishes a class that can be used as a custom allocator. A call to its \texttt{staticNew} method will return a pointer into a previously allocated chunk of memory. The next call to \texttt{staticNew} will return another pointer inside the same chunk that is adjacent to the previously returned memory segment. This memory management is equivalent to a stack. Once the chunk is filled, a new one is allocated with a call to the system allocator. The old chunk is not stored and there is no possibility to access it again with mara (but the previously shared pointers remain valid). Each object of the mara class manages disjoint parts of the heap.

With memory allocated by mara we can very easily distribute LoLAs net state memory, to make it thread safe. With the \texttt{Hashing\-Wrapper\-Store} we already have a method to store net states in independent \texttt{PrefixTreeStore}s (inside the buckets). The access to them is already restricted to one thread at a time, while other buckets still can be accessed by other threads. This means that we do not need to consider parallelization issues inside the buckets of the \texttt{PrefixTreeStore}. All memory access inside, can only be done by a single thread. To use mara as an allocator, we just have to create one instance of the mara class and substitute all calls to \texttt{malloc}, \texttt{calloc} and \texttt{new} inside the \texttt{PrefixTreeStore} with a call to maras \texttt{staticNew} method. Because calls to delete and free will now produce undefined behavior, these calls have to be removed as well. The operating system will free the memory on program termination. After these steps, all heap allocation for the net states is wrapped by mara. 

Altering LoLA in that way we now can start another benchmark to see if the changes impact LoLAs performance. 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{pictures/perfReportMara.png}
    \caption{perf report output of LoLA with mara integration}
    \label{perf_record_mara}
\end{figure}

\section{Results}
\subsection{Profiling}
To compare the old and the new implementation, we have to rerun our benchmark. Figure \ref{perf_record_mara} shows the output of the same mara call we used in chapter \ref{first_perf_results}.

We can see that the data changed in many ways. The \texttt{searchAndInsert} and the \texttt{getFirelist} methods now contribute to roughly the same amount of samples. They are also the functions with the highest 'Self' time. This means most samples were taken directly in this functions and not in one of their sub functions.

The most important fact however is that the calls to \texttt{libcalloc} are completely gone, meaning they are under the significance threshold or where not sampled at all. Instead we can see that maras \texttt{staticNew} method contributes to 0.4\% of all samples.

It seems that the integration of mara has a great impact on LoLAs performance. We seem to have greatly improved the way mara uses the heap. But we do not get any information on the execution time and the thread scaling from the profiling data. To draw any conclusion we have to make additional macro benchmarks and see how the changes affect the execution time.

\subsection{Macrobenchmark}
\input{pictures/MacroNOBucketsAll}
There are different questions that are of interest after changing the implementations. To address a variety of them, multiple tests were issued. We can divide them into three categories:
\begin{enumerate}
    \item Single thread performance,
    \item Implementation comparison and
    \item Bucket influence.
\end{enumerate}
The single thread performance will be compared without the use of the \texttt{Hashing\-Wrapper\-Store}. This way we can observe any efficiency differences that are caused by mara or the compare and swap (CAS) synchronization.
To have insight into the thread scaling we will compare all implementations with use of the \texttt{Hashing\-Wrapper\-Store} combined with different amounts of threads.
And to see how the amount of used buckets influences the results, we will look at just the implementations that are scaling with the threads and run them with a different amount of buckets.

We will investigate all implementation changes. The preexisting unchanged implementation will be called 'PThread', The implementation with the changed synchronization method will be called 'CAS' and the implementation with the integration of mara will be called 'Mara PThread'. Additionally we will look at an implementation where the CAS synchronization and the mara integration was used. It will be called 'Mara CAS'.

In this case we will run the benchmarks on ebro because it can access up to 64 threads. However we will limit the thread count to a maximum of 50 so that other tasks are unlikely to interfere with our benchmark.

The benchmark values will be an average of 10 runs, so that performance deviations are compensated.

In the single thread benchmark - shown in figure \ref{SingleThreadBenchmark} - a quite homogeneous characteristic can be observed. The difference between the CAS and the Pthread implementation is insignificant. But we can observe a speedup between the mara and non-mara implementations. With a single thread the pthread mutexes, still seem to work slightly faster than the compare and swap approach.

\input{pictures/Macro100Buckets}
The benchmark for the implementation comparison shows that the original implementation is quite consistent between the used threads. This is due to bugs and will be explained further in chapter \ref{evaluation}. We can see again that the CAS implementation nether really outperforms the original implementation. Only with the addition that at a very high thread count, the execution time rises again to very high levels. Both mara implementations however, are able to outperform the PThread implementation at a certain thread count. Even though they are performing considerably worse on lower thread counts.

\input{pictures/MacroMultipleBuckets}
But to utilize a maximum of the parallelization capabilities, the memory has to be used efficiently. Increasing the number of threads, will increase the probability of concurrent access of the same bucket. In such a case the bucket will grant access to a single thread and will lock the access for others. Thus a higher thread count might ultimately cause an - again - decreasing performance.

We can decrease the chances of concurrent access by increasing the number of used buckets. Therefore figure \ref{MaraBucketComparison} shows the performance of the mara implementations with a different number of buckets. We can observe a general performance gain, with a higher number of buckets. Even single-threaded. The difference slowly diminishes with more threads. However, a greater bucket count preserves a better thread scaling. We can observe a further decreasing of the execution time, with more buckets, on a higher thread count. Whereas the execution time of CAS with 100 buckets increases again between 20 and 50 used threads.

At this point we can summarize that we have affected the performance with the new implementations. In some cases even for the better. In the next chapter we will evaluate the data in more detail.

\subsection{Evaluation}
\label{evaluation}
We saw that the new implementations perform differently in the results of the benchmark. We can now analyze what the causes of these differences are.

But before that, it should be stated again that the benchmarks where run on a single Petri net and on a single machine. While the used hardware should hardly influence the relative performance differences, the test data can have a major effect. For example a Petri net, where always at most on transition is fireable in each state, cannot be searched in parallel by the current algorithm. In such a case we would see no benefit by using multiple threads and parallelization would certainly perform worse due to its synchronization overhead.

Additionally the use of LoLA with the \texttt{--check=full} parameter is by far not the only possible use of LoLA. It is probably most used with a predicate to check. In that case, the Petri net is often reduced to a smaller - with respect to the predicate - equivalent net, before the search is done. This again can influence the number of fireable transitions and therefore the potential of the parallel search.

That said, the following should be seen as a constrictive comparison between the sequential and parallel capabilities of LoLA, not as an exhaustive one.

\subsubsection{Single thread performance}
\label{evalSingleThread}
We have two different results on the single thread performance of LoLA. If the \texttt{Hashing\-Wrapper\-Store} is used all new implementations perform worse than the initial implementation. If it is not used the mara implementations perform better.

The performance gain in the latter can be explained exactly by the use of mara. Benchmarks in the development of mara showed that it is faster than a call to \texttt{malloc}, because it can reduce an allocation call to basically three stack operations:
\begin{enumerate}
    \item Save the current stack pointer
    \item Increase the stack pointer by the size of the requested data
    \item Return the saved pointer
\end{enumerate}
Because LoLA requests a lot of heap space, this adds up to a significant amount of time.

But when we introduce the \texttt{Hashing\-Wrapper\-Store} with its buckets, the performance of all new implementations drop considerably, while the initial implementation gains performance. 

The performance gain can be explained by the concept of the \texttt{Hashing\-Wrapper\-Store}. When we distribute all states evenly between the buckets, we can do an informed search later because we know the bucket associated with our hash value. Since every insert of a net state implies a previous search to avoid duplicates, it is possible to reduce the execution time with a suitable amount of buckets.

The performance loss can be explained by the implementation of the \texttt{Hashing\-Wrapper\-Store}. The original implementation was not guarded by mutexes and therefore not thread safe. Additionally the bucket count from the lola call was ignored and a hard-coded value was used instead. This suggests that the implementation was incomplete and the use of buckets was experimental. For the use in the new implementations, the \texttt{Hashing\-Wrapper\-Store} had to be altered. Now, before a bucket will be accessed via hash, a mutex has to be passed to prevent concurrent access and a modulo check is done to assure that a bucket to insert exists. These new operations can accumulate a lot of time with frequent use by LoLA.

However, aside from the synchronization issues it seems that mara has the potential to speed up the execution in general. The heap allocation is independent of the depth-first search and the optimization techniques incorporated by LoLA. 

Also it might be possible to affect the memory space consumption with mara. Though not benchmarked explicitly, while testing the implementations during development, a considerable memory reduction was observed. While the 1000 philosophers caused a premature termination of the tests with the \texttt{Hashing\-Wrapper\-Store}, the mara integration enabled LoLA to finish the test successful under the right circumstances. With the approximately 17GB of RAM on the VM, the original implementation terminated at about the half of net states to discover. But with a chunk size of 500MB it was possible to fit all net states in the memory. It can be assumed that this is a saving of multiple gigabytes and might be a matter that is worth investigating more in depth.

To address the worse performance with the changes in the \texttt{Hashing\-Wrapper\-Store}, it might be possible to reduce the execution time by checking with how many threads LoLA runs. A condition to skip the mutex blocks when using only one thread, or a completely new class that is loaded dynamically single-threaded, can be implemented. Anyway, the performance gap between the hashed and unhashed execution of the PThread implementation suggests that this matter can buy in execution time at expense for memory space.

\subsubsection{Thread scaling}
The main goal of this work was to achieve a scaling with the threads. As we can see from the benchmark data, the initial PThread implementation is not affected by the number of threads at all. The reason for this is that - due to a bug - the configured thread count is completely ignored. 

The bug is caused by differing function signatures. LoLA uses different classes for the depth-first search when using one thread, and when using more than one thread. If LoLA is initialized with one thread it uses the \texttt{DFSExploration} class. With more threads it uses the \texttt{ParallelExploration} class (that is derived from the \texttt{DFSExploration}). To start the search, the \texttt{depth\_first} method is called with this signature:
\lstset{language=C++, frame=none, caption={Signature in \texttt{DFSExploration}}, stepnumber=5, backgroundcolor=\color{verylightgray}}
\begin{lstlisting}
bool depth_first SimpleProperty, NetState, Store<void>, Firelist, int)
\end{lstlisting}
This method is implemented in the \texttt{DFSExploration} class. But in the \texttt{Parallel\-Exploration} class the method is implemented with the signature: 
\lstset{language=C++, frame=none, caption={Signature in \texttt{ParallelExploration}},stepnumber=5, backgroundcolor=\color{verylightgray}}
\begin{lstlisting}
bool depth_first SimpleProperty, NetState, Store<void>, Firelist, threadid_t)
\end{lstlisting}
 This probably causes an additional entry in the objects virtual table, so that the \texttt{depth\_first} method appears multiple times with the two different signatures. Since LoLA calls the search method in the \texttt{ParallelExploration} with the first signature, the (sequential) base implementation is used instead of the new one. The bug is fixed simply by changing the child signatures \texttt{threadid\_t} to an \texttt{int}.

If this bug is resolved we can observe an increasing performance with an increasing amount of threads. At least in the lower thread range. With 50 threads the CAS implementation performs worst. Since the test was run with 100 buckets, this behavior can be explained with an increased blocking of threads not only on entering a bucket, but also on allocating heap memory with \texttt{calloc}. The latter seems to have a greater effect if we compare the results with the mara implementations.

After switching to mara, the allocator blocking seems to be gone as expected. The mara implementations are the first to beat the single-threaded implementation with a high amount of threads. This leads to the assumption that allocating heap space with the system allocator was the main cause of the previous bottleneck. We can even see a linear scaling in the range from two to four threads. Only between one and two threads, the performance does not split in half, because the single-threaded algorithm does use the original \texttt{DFSExploration} which comes with much less synchronization overhead.

In contrast to the first impression it is even possible that the linear scaling reaches the higher thread range. Eventually LoLA has a single-threaded build up and teardown phase. This phases are included in LoLAs internal measurement and therefore are included in the results. But since we do not have reliable data for that phases, this matter will remain speculative for now.

\subsubsection{Bucket influence}
We can see continuous improvement with the use of additional buckets in the mara implementations. 

One reason for that is probably the decreased chance for concurrent access. This gets more important with increasing threads. In fact we can see a slight decrease of the mara CAS performance between 20 and 50 threads that seems to be compensated with the use of more buckets.

But the increasing single thread performance cannot be explained by fewer bucket conflicts. As described in \ref{evalSingleThread}, this behavior is probably caused by a reduced store search time. Because LoLA distributes all states in the buckets and knows in which bucket to search for a given net state, the overall search time is decreasing.

However, the extent of both effects is not reflected in the data. To evaluate this matter a macro benchmark does not suffice.
