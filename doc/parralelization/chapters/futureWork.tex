% !TEX root = ../main.tex
\chapter{Future Work}
The results show that it is possible to further speed up LoLAs performance. But they draw not the complete picture. Using different optimization methods and other petri nets, the performance gain most certainly ease noticeably.

To classify the extend of this behavior, further benchmarks should be evaluated. They will show if the observed scaling is applicable in a productive environment. Additional a memory benchmark might be of interest, that compares memory load with the mara implementation (depending on the chunk size) in comparison to the old implementation.

However, LoLAs potential for parallel and sequential execution is by far not exhausted. For example, the new implementation of the \inline{HashingWrapperStore} seems to be much slower than the previous version. As stated in chapter \ref{evalSingleThread}, a concept for a single threaded use might decrease the execution time considerably. A special subclass that is specialized for - and only used on - single threaded execution might be a good way to achieve this goal.

As discussed in chapter \ref{codeBase} the implementation of the \inline{ParallelExploration} class seems to be based on the single threaded base class. With this approach all threads used for the exploration have the same job. They ara also accessing equally the same global storage. A more specialized approach might deliver even better scaling effects or reduce the overhead that is needed for the synchronization.

For example, currently the store is accessed via the \inline{searchAndInsert} method. As the name implies, this method not only searches the store for a net state - which requires reading access, it also inserts the state into the store if it was not found - which requires writing access. If it is possible to divide this two operations in a thread safe manner, it might be possible to reduce the chance of blocking mutexes. Especially if we consider that the search depends on explored states, whereas inserting can be done in constant time.

Another possible improvement might be a master thread that querries for left over work by worker threads, that can than be handed over to idle worker threads. This way it can be avoided to check for idle threads on each search cycle like it is done currently.

However this would be solutions that are bound to the current structure of LoLA. It might be worth to completely reimplement the parallel search. The recent development in dfs is tying to introduce efficient parallel extensions of existing algorithms \cite{bloemen2016multi} \cite{holzmann2008stack}. With an algorithm and a data structure that is well tested and specifically designed for parallel use it might be possible to achieve a better performance for more use cases.